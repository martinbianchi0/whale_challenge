{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cb4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from src.models.classification import *\n",
    "from src.models.generative import *\n",
    "from src.preprocessing import *\n",
    "from src.display import *\n",
    "from src.metrics import *\n",
    "from src.utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import KFold, ParameterGrid, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cbadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_CONFIG = {\n",
    "    \"SR\": 2000,              # sampling rate\n",
    "    \"FFT_SAMPLES\": 256,      # tamaño de la ventana\n",
    "    \"HOP_LENGTH\": 63,        # salto entre frames\n",
    "    \"MEL_BINS\": 64,          # frequency bins (resolución)\n",
    "    \"MAX_FREQ\": 600          # máxima frecuencia para los espectrogramas\n",
    "}\n",
    "\n",
    "DATA_AUG_CONFIG = {\n",
    "    'TIME_STRETCH_FACTORS': [0.9, 1.1],     # ±10%\n",
    "    'PITCH_SHIFTS': [-2, 2],                # ±2 semitonos\n",
    "    'NOISE_LEVEL': 0.001                    # ruido leve\n",
    "}\n",
    "\n",
    "SEED = 3    # semilla para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86417ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clip_name', 'label', 'filepath', 'audio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/whale-detection-challenge/data/train'\n",
    "test_dir = 'data/whale-detection-challenge/data/test'\n",
    "labels_dir = 'data/whale-detection-challenge/data/train.csv'\n",
    "\n",
    "audio_df, labels_df, test_files = load_data(train_dir, test_dir, labels_dir, sampling_rate=SPEC_CONFIG['SR'])\n",
    "print(audio_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320c87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(audio_df, 'audio') # chequear esta linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc5cb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(audio_df, test_size=0.2, random_state=SEED, stratify=audio_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f6f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f794c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mel_spec = []\n",
    "for whale_audio in train_df['audio']: # emprolijar esto de abajo\n",
    "    mel = get_melspectrogram(whale_audio, SPEC_CONFIG['SR'], SPEC_CONFIG['FFT_SAMPLES'], SPEC_CONFIG['HOP_LENGTH'], SPEC_CONFIG['MEL_BINS'], SPEC_CONFIG['MAX_FREQ']) # shape: (64, 64)\n",
    "    mel = mel[np.newaxis, :, :] # shape: (1, 64, 64)\n",
    "    train_mel_spec.append(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7721389",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mel_spec = []\n",
    "for whale_audio in val_df['audio']: # emprolijar esto de abajo\n",
    "    mel = get_melspectrogram(whale_audio, SPEC_CONFIG['SR'], SPEC_CONFIG['FFT_SAMPLES'], SPEC_CONFIG['HOP_LENGTH'], SPEC_CONFIG['MEL_BINS'], SPEC_CONFIG['MAX_FREQ']) # shape: (64, 64)\n",
    "    mel = mel[np.newaxis, :, :] # shape: (1, 64, 64)\n",
    "    val_mel_spec.append(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f21bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_mel_spec)\n",
    "X_val = np.array(val_mel_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b90ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -22.267026901245117, std: 10.529284487233888\n"
     ]
    }
   ],
   "source": [
    "X_train_std, X_val_std, train_mean, train_std = standarize_train_val(X_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7361ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "611b6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_std, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_std, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd3a7a",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a99d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "\n",
    "def get_class_weights(y):\n",
    "    # Simple helper to compute class weights\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    import numpy as np\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, output_dim=2):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else\n",
    "                                   'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # (1, 64, 64) -> (16, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                         # (16, 64, 64) -> (16, 32, 32)\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), # -> (32, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                          # -> (32, 16, 16)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # -> (64, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                          # -> (64, 8, 8)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=1e-5,\n",
    "                    early_stopping_patience=None, use_class_weights=True):\n",
    "        self.to(self.device)\n",
    "\n",
    "        if use_class_weights:\n",
    "            y_train = train_loader.dataset.tensors[1].cpu().numpy().flatten()\n",
    "            weights = get_class_weights(y_train).to(self.device)\n",
    "            loss_function = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        best_val_loss, patience_counter = float('inf'), 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            val_loss, val_acc, val_f1, val_auc = self.evaluate(val_loader, return_metrics=True)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | '\n",
    "                  f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "\n",
    "            if early_stopping_patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = copy.deepcopy(self.state_dict())\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= early_stopping_patience:\n",
    "                        print(\"Early stopping triggered (val loss did not improve).\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            self.load_state_dict(best_model_state)\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def evaluate(self, val_loader, return_metrics=False):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels, all_probs, all_preds = [], [], []\n",
    "\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels, all_probs)\n",
    "        except ValueError:\n",
    "            auc = 0.0\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        if return_metrics:\n",
    "            return avg_val_loss, acc * 100, f1, auc\n",
    "\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {acc * 100:.2f}%, F1: {f1:.4f}, AUC: {auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e57d42af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.3247 | Val Loss: 0.2768 | Val Acc: 88.03% | Val F1: 0.7896 | Val AUC: 0.9600\n",
      "Epoch 2/50 | Train Loss: 0.2154 | Val Loss: 0.2044 | Val Acc: 90.23% | Val F1: 0.8181 | Val AUC: 0.9702\n",
      "Epoch 3/50 | Train Loss: 0.1911 | Val Loss: 0.1987 | Val Acc: 90.87% | Val F1: 0.8281 | Val AUC: 0.9738\n",
      "Epoch 4/50 | Train Loss: 0.1760 | Val Loss: 0.1982 | Val Acc: 91.22% | Val F1: 0.8330 | Val AUC: 0.9750\n",
      "Epoch 5/50 | Train Loss: 0.1652 | Val Loss: 0.1888 | Val Acc: 91.78% | Val F1: 0.8409 | Val AUC: 0.9755\n",
      "Epoch 6/50 | Train Loss: 0.1522 | Val Loss: 0.1907 | Val Acc: 92.35% | Val F1: 0.8491 | Val AUC: 0.9758\n",
      "Epoch 7/50 | Train Loss: 0.1408 | Val Loss: 0.2061 | Val Acc: 92.42% | Val F1: 0.8497 | Val AUC: 0.9756\n",
      "Epoch 8/50 | Train Loss: 0.1296 | Val Loss: 0.1988 | Val Acc: 92.23% | Val F1: 0.8443 | Val AUC: 0.9743\n",
      "Epoch 9/50 | Train Loss: 0.1166 | Val Loss: 0.2205 | Val Acc: 91.85% | Val F1: 0.8427 | Val AUC: 0.9752\n",
      "Epoch 10/50 | Train Loss: 0.0990 | Val Loss: 0.2249 | Val Acc: 91.60% | Val F1: 0.8383 | Val AUC: 0.9742\n",
      "Early stopping triggered (val loss did not improve).\n",
      "Validation Loss: 0.1888, Accuracy: 91.78%, F1: 0.8409, AUC: 0.9755\n"
     ]
    }
   ],
   "source": [
    "CNN = SpectrogramCNN(output_dim=2)\n",
    "CNN.train_model(train_loader, val_loader, epochs=50, early_stopping_patience=5)\n",
    "CNN.evaluate(val_loader) # el criterio de early stopping deberia ser por loss de val, no por f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da19c02",
   "metadata": {},
   "source": [
    "#### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac36dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIATIONAL AUTOENCODER MODEL\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc = nn.Sequential(\n",
    "            # (1, 64, 64) -> (32, 32, 32)\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (32, 32, 32) -> (64, 16, 16)\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (64, 16, 16) -> (128, 8, 8)\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (128, 8, 8) -> (256, 4, 4)\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.dec = nn.Sequential(\n",
    "            # (256, 4, 4) -> (128, 8, 8)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (128, 8, 8) -> (64, 16, 16)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (64, 16, 16) -> (32, 32, 32)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (32, 32, 32) -> (1, 64, 64)\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            # Sin activación final para MSE loss (usá Sigmoid para [0,1])\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_fc(z).view(-1, 256, 4, 4)\n",
    "        return self.dec(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "        \n",
    "    def bvae_loss(self, recon_x, x, mu, logvar, beta=1.0):\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + beta * kld_loss, recon_loss, kld_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6285a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✔] Modelo cargado desde: saved_models/bvae.pt\n"
     ]
    }
   ],
   "source": [
    "def load_model(path=\"saved_models/bvae.pt\"):\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "    model = BetaVAE().to(device)\n",
    "    state_dict = torch.load(path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(f\"[✔] Modelo cargado desde: {path}\")\n",
    "    return model\n",
    "\n",
    "BVAE = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01beba34",
   "metadata": {},
   "source": [
    "#### DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcf98d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De los 24000 datos que se usan para entrenar: 5622 son de ballena y 18378 son de ruido, por ende falta completar con 12756 audios de ballena.\n"
     ]
    }
   ],
   "source": [
    "datos_totales = len(y_train)\n",
    "audios_ballena = np.count_nonzero(y_train)\n",
    "audios_ruido = datos_totales - audios_ballena\n",
    "ballenas_faltantes = audios_ruido - audios_ballena\n",
    "\n",
    "print(f'De los {datos_totales} datos que se usan para entrenar: {audios_ballena} son de ballena y {audios_ruido} son de ruido, por ende falta completar con {ballenas_faltantes} audios de ballena.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afecfb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (24000, 1, 64, 64)\n",
      "y_train shape: (24000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {(X_train.shape)}')\n",
    "print(f'y_train shape: {(y_train.shape)}')\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6deeccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_synth = ballenas_faltantes\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(n_synth, latent_dim).to(device)\n",
    "    synth_specs = BVAE.decode(z).cpu().numpy()  # shape: (n_synth, 1, 64, 64)\n",
    "    synth_specs = synth_specs * 11.347515106201172 + -21.889528274536133 # estos son el mean y std con los q se estandarizaron los datos de train del vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d5324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_labels = np.ones(n_synth, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dab533bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = np.concatenate([X_train, synth_specs], axis=0)\n",
    "y_train_aug = np.concatenate([y_train, synth_labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa5f169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug_std = (X_train_aug - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b01f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_X_train_tensor = torch.tensor(X_train_aug_std, dtype=torch.float32)\n",
    "aug_y_train_tensor = torch.tensor(y_train_aug, dtype=torch.long)\n",
    "\n",
    "aug_train_loader = DataLoader(TensorDataset(aug_X_train_tensor, aug_y_train_tensor), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6146579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.3422 | Val Loss: 0.2923 | Val Acc: 87.88% | Val F1: 0.7838 | Val AUC: 0.9581\n",
      "Epoch 2/50 | Train Loss: 0.2228 | Val Loss: 0.2062 | Val Acc: 90.25% | Val F1: 0.8175 | Val AUC: 0.9702\n",
      "Epoch 3/50 | Train Loss: 0.1933 | Val Loss: 0.2230 | Val Acc: 91.03% | Val F1: 0.8291 | Val AUC: 0.9721\n",
      "Epoch 4/50 | Train Loss: 0.1788 | Val Loss: 0.2111 | Val Acc: 91.47% | Val F1: 0.8344 | Val AUC: 0.9734\n",
      "Epoch 5/50 | Train Loss: 0.1700 | Val Loss: 0.1777 | Val Acc: 92.50% | Val F1: 0.8479 | Val AUC: 0.9742\n",
      "Epoch 6/50 | Train Loss: 0.1607 | Val Loss: 0.1792 | Val Acc: 92.08% | Val F1: 0.8438 | Val AUC: 0.9753\n",
      "Epoch 7/50 | Train Loss: 0.1482 | Val Loss: 0.1871 | Val Acc: 92.85% | Val F1: 0.8545 | Val AUC: 0.9756\n",
      "Epoch 8/50 | Train Loss: 0.1364 | Val Loss: 0.2164 | Val Acc: 91.07% | Val F1: 0.8306 | Val AUC: 0.9732\n",
      "Epoch 9/50 | Train Loss: 0.1219 | Val Loss: 0.2302 | Val Acc: 91.90% | Val F1: 0.8415 | Val AUC: 0.9743\n",
      "Epoch 10/50 | Train Loss: 0.1108 | Val Loss: 0.2050 | Val Acc: 92.58% | Val F1: 0.8425 | Val AUC: 0.9731\n",
      "Early stopping triggered (val loss did not improve).\n",
      "Validation Loss: 0.1777, Accuracy: 92.50%, F1: 0.8479, AUC: 0.9742\n"
     ]
    }
   ],
   "source": [
    "CNN = SpectrogramCNN(output_dim=2)\n",
    "CNN.train_model(train_loader, val_loader, epochs=50, early_stopping_patience=5)\n",
    "CNN.evaluate(val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
