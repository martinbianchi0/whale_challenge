{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd90764",
   "metadata": {},
   "source": [
    "____\n",
    "__Universidad de San Andrés__<br/>\n",
    "__Machine Learning__<br/>\n",
    "__Modelado probabilistico de audios de ballenas y estudio de transfer learning__<br/>\n",
    "__Martin Bianchi y Federico Gutman__\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baffab2",
   "metadata": {},
   "source": [
    "### Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2aa0fa",
   "metadata": {},
   "source": [
    "### NOTAS\n",
    "- ver repo para preprocesar espectrogrma\n",
    "    - ver de reducir el y-lim del spectrograma -> averiguar bien entre que frecuencias se mueven los cantos de ballena\n",
    "- cambiar noise a no right whale\n",
    "- cambiar el idioma a español\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f75ba",
   "metadata": {},
   "source": [
    "### Definimos algunas variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc552f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3            # semilla para reproductibilidad\n",
    "SR = 2000           # sampling rate\n",
    "N_FFT = 256         # tamaño de la ventana\n",
    "HOP_LENGTH = 64     # salto entre frames\n",
    "N_MELS = 50         # frequency bins (resolución)\n",
    "MAX_FREQ = 600      # máxima frecuencia para los espectrogramas --ponerlo en 500--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971bf89",
   "metadata": {},
   "source": [
    "### Cargamos los datos y los visualizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72587866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path:str, test_path:str, labels_path:str, sampling_rate=SR):\n",
    "    test_files = [f for f in os.listdir(test_path) if f.endswith('.aiff')]\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "    audio_df = labels_df.copy()\n",
    "    audio_df['filepath'] = audio_df['clip_name'].apply(lambda x: os.path.join(train_path, x))\n",
    "    audio_df['audio'] = audio_df['filepath'].apply(lambda path: librosa.load(path, sr=sampling_rate)[0])\n",
    "\n",
    "    return audio_df, labels_df, test_files\n",
    "\n",
    "def display_random_samples(dataset:pd.DataFrame, sampling_rate=SR):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # SAMPLES\n",
    "    whale_sample = dataset[dataset['label'] == 1].sample(5)['audio']\n",
    "    noise_sample = dataset[dataset['label'] == 0].sample(5)['audio']\n",
    "\n",
    "    # AUDIO\n",
    "    print('right whale call random audio sample')\n",
    "    display(Audio(np.array(whale_sample.iloc[0]), rate=sampling_rate*1.5))\n",
    "    print('\\nno whale random audio sample')\n",
    "    display(Audio(np.array(noise_sample.iloc[0]), rate=sampling_rate*1.5))\n",
    "\n",
    "    # SOUND-WAVE\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 5))\n",
    "\n",
    "    for i in range(5):\n",
    "        axes[0, i].plot(np.array(whale_sample.iloc[i]))\n",
    "        axes[0, i].set_title('Whale')\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "\n",
    "        axes[1, i].plot(np.array(noise_sample.iloc[i]), color='#FF6961')\n",
    "        axes[1, i].set_title('No Whale')\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # MULTIPLE SPECTROGRAMS\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "    for i in range(5):\n",
    "        whale_sample_spectrogram = get_melspectrogram(whale_sample.iloc[i])\n",
    "        img0 = librosa.display.specshow(whale_sample_spectrogram, sr=sampling_rate, hop_length=HOP_LENGTH, ax=axes[0, i], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "        axes[0, i].set_title('Whale')\n",
    "        axes[0, i].set_ylim([0, MAX_FREQ])\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "        axes[0, i].set_xlabel('')\n",
    "        axes[0, i].set_ylabel('')\n",
    "\n",
    "        noise_sample_spectrogram = get_melspectrogram(noise_sample.iloc[i])\n",
    "        img1 = librosa.display.specshow(noise_sample_spectrogram, sr=sampling_rate, hop_length=HOP_LENGTH, ax=axes[1, i], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "        axes[1, i].set_title('No Whale')\n",
    "        axes[1, i].set_ylim([0, MAX_FREQ])\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "        axes[1, i].set_xlabel('')\n",
    "        axes[1, i].set_ylabel('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # TWO SPECTROGRAMS\n",
    "    whale_sample2 = dataset[dataset['label'] == 1].sample(1, random_state=SEED+1)['audio']\n",
    "    noise_sample2 = dataset[dataset['label'] == 0].sample(1, random_state=SEED+1)['audio']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "    whale_sample2_spectrogram = get_melspectrogram(whale_sample2.iloc[0])\n",
    "    img0 = librosa.display.specshow(whale_sample2_spectrogram, sr=sampling_rate, hop_length=HOP_LENGTH, ax=axes[0], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[0].set_title('Whale')\n",
    "    axes[0].set_ylim([0, MAX_FREQ])\n",
    "    fig.colorbar(img0, ax=axes[0], format=\"%+2.0f dB\")\n",
    "\n",
    "    noise_sample2_spectrogram = get_melspectrogram(noise_sample2.iloc[0])\n",
    "    img1 = librosa.display.specshow(noise_sample2_spectrogram, sr=sampling_rate, hop_length=HOP_LENGTH, ax=axes[1], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[1].set_title('No Whale')\n",
    "    axes[1].set_ylim([0, MAX_FREQ])\n",
    "    fig.colorbar(img1, ax=axes[1], format=\"%+2.0f dB\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def normalize(dataset:pd.DataFrame, column:str):\n",
    "    dataset[column] = dataset[column].apply(lambda x: x / np.max(np.abs(x)))\n",
    "\n",
    "def get_signal_energy(dataset:pd.DataFrame, column:str):\n",
    "    energy = []\n",
    "    df = dataset.copy()\n",
    "    for x in df[column]:\n",
    "        energy.append(np.sum(np.square(x)))\n",
    "    df['energy'] = energy\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_melspectrogram(sample:pd.DataFrame, sampling_rate=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=np.array(sample), sr=sampling_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmax=MAX_FREQ)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spectrogram\n",
    "\n",
    "def get_all_mel_spectrograms(audio_df, sampling_rate=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS):\n",
    "    mel_specs = []\n",
    "    for audio in audio_df['audio']:\n",
    "        mel = get_melspectrogram(audio, sampling_rate, n_fft, hop_length, n_mels)\n",
    "        mel_specs.append(mel.flatten())\n",
    "    return np.array(mel_specs)\n",
    "\n",
    "def show_class_balance(dataset:pd.DataFrame):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    ax = sns.countplot(\n",
    "        x='label',\n",
    "        data=dataset,\n",
    "        palette={0: '#FF6961', 1: plt.rcParams['axes.prop_cycle'].by_key()['color'][0]},\n",
    "        hue='label',\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Noise', 'Whale'])\n",
    "    ax.legend(['Noise', 'Whale'], title='Class')\n",
    "    plt.show()\n",
    "\n",
    "def extract_time_acoustic_features(dataset:pd.DataFrame):\n",
    "    new_features = []\n",
    "    for audio in dataset['audio']:\n",
    "        audio_np = np.array(audio)\n",
    "        acoustic_features = {\n",
    "            'rms_energy': np.mean(librosa.feature.rms(y=audio_np)),\n",
    "            'zcr': np.mean(librosa.feature.zero_crossing_rate(y=audio_np)),\n",
    "        }\n",
    "        new_features.append(acoustic_features)\n",
    "    features_df = pd.DataFrame(new_features)\n",
    "    return features_df\n",
    "\n",
    "def extract_frequency_acoustic_features(dataset:pd.DataFrame):\n",
    "    new_features = []\n",
    "    for audio in dataset['audio']:\n",
    "        audio_np = np.array(audio)\n",
    "        acoustic_features = {\n",
    "            'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=audio_np)),\n",
    "            'spectral_bandwidth': np.mean(librosa.feature.spectral_bandwidth(y=audio_np)),\n",
    "            'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=audio_np)),\n",
    "            'spectral_flatness': np.mean(librosa.feature.spectral_flatness(y=audio_np))\n",
    "        }\n",
    "        new_features.append(acoustic_features)\n",
    "    features_df = pd.DataFrame(new_features)\n",
    "    return features_df\n",
    "\n",
    "def extract_acoustic_features(dataset:pd.DataFrame):\n",
    "    time_features = extract_time_acoustic_features(dataset)\n",
    "    frequency_features = extract_frequency_acoustic_features(dataset)\n",
    "\n",
    "    new_df = pd.concat([dataset.reset_index(drop=True), time_features], axis=1)\n",
    "    new_df = pd.concat([new_df.reset_index(drop=True), frequency_features], axis=1)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def display_features_boxplots(audio_features_df:pd.DataFrame):\n",
    "    time_features = ['rms_energy', 'zcr']\n",
    "    freq_features = ['spectral_centroid', 'spectral_bandwidth', 'spectral_rolloff', 'spectral_flatness']\n",
    "\n",
    "    # TIME\n",
    "    fig, axes = plt.subplots(1, len(time_features), figsize=(5 * len(time_features), 5))\n",
    "    if len(time_features) == 1:\n",
    "        axes = [axes]\n",
    "    for i, feature in enumerate(time_features):\n",
    "        sns.boxplot(x='label', y=feature, data=audio_features_df, ax=axes[i], showfliers=False)\n",
    "        axes[i].set_title(f'{feature} by Class')\n",
    "        axes[i].set_xlabel('Class')\n",
    "        axes[i].set_xticks([0, 1])\n",
    "        axes[i].set_xticklabels(['Noise', 'Whale'])\n",
    "\n",
    "    plt.suptitle('Time Domain Features by Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # FREQUENCY\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, feature in enumerate(freq_features):\n",
    "        sns.boxplot(x='label', y=feature, data=audio_features_df, ax=axes[i], showfliers=False)\n",
    "        axes[i].set_title(f'{feature} by Class')\n",
    "        axes[i].set_xlabel('Class')\n",
    "        axes[i].set_xticks([0, 1])\n",
    "        axes[i].set_xticklabels(['Noise', 'Whale'])\n",
    "\n",
    "    plt.suptitle('Frequency Domain Features by Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pca(dataset:pd.DataFrame, pca_arr):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = sns.scatterplot(\n",
    "        x=pca_arr[:,0], \n",
    "        y=pca_arr[:,1], \n",
    "        hue=dataset['label'], \n",
    "        palette={0: '#FF6961', 1: '#1f77b4'}\n",
    "    )\n",
    "    plt.title('PCA of Mel Spectrograms')\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    # Fix legend labels and colors\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "    scatter.legend(handles=handles, title='Class', labels=['Noise', 'Whale'])\n",
    "    plt.show()\n",
    "\n",
    "def plot_average_spectrograms(audio_df:pd.DataFrame):\n",
    "    whale_spectrograms = []\n",
    "    noise_spectrograms = []\n",
    "    for audio, label in zip(audio_df['audio'], audio_df['label']):\n",
    "        spectrogram = get_melspectrogram(audio)\n",
    "        if label == 1:\n",
    "            whale_spectrograms.append(spectrogram)\n",
    "        else: \n",
    "            noise_spectrograms.append(spectrogram)\n",
    "    \n",
    "    whale_spectrograms = np.array(whale_spectrograms)\n",
    "    noise_spectrograms = np.array(noise_spectrograms)\n",
    "\n",
    "    whale_average_spectrogram = whale_spectrograms.mean(axis=0)\n",
    "    noise_average_spectrogram = noise_spectrograms.mean(axis=0)\n",
    "    average_differences_spectrogram = whale_average_spectrogram - noise_average_spectrogram\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "    img0 = librosa.display.specshow(whale_average_spectrogram, sr=SR, hop_length=HOP_LENGTH, ax=axes[0], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[0].set_title('Whale Call Average Spectrogram', pad=20)\n",
    "    axes[0].set_ylim([0, MAX_FREQ])\n",
    "    # axes[0].set_xticks([])\n",
    "    # axes[0].set_yticks([])\n",
    "    fig.colorbar(img0, ax=axes[0], format=\"%+2.0f dB\")\n",
    "\n",
    "    img1 = librosa.display.specshow(noise_average_spectrogram, sr=SR, hop_length=HOP_LENGTH, ax=axes[1], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[1].set_title('No Whale Call Average Spectrogram', pad=20)\n",
    "    axes[1].set_ylim([0, MAX_FREQ])\n",
    "    # axes[1].set_xticks([])\n",
    "    # axes[1].set_yticks([])\n",
    "    fig.colorbar(img1, ax=axes[1], format=\"%+2.0f dB\")\n",
    "\n",
    "    img2 = librosa.display.specshow(average_differences_spectrogram, sr=SR, hop_length=HOP_LENGTH, ax=axes[2], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[2].set_title('Average Differences Spectrogram', pad=20)\n",
    "    axes[2].set_ylim([0, MAX_FREQ])\n",
    "    # axes[2].set_xticks([])\n",
    "    # axes[2].set_yticks([])\n",
    "    fig.colorbar(img2, ax=axes[2], format=\"%+2.0f dB\")\n",
    "\n",
    "    # 3 subplots -> average spectrogram whale, noise, resto y consigo diferencias\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/whale-detection-challenge/data/train'\n",
    "test_dir = 'data/whale-detection-challenge/data/test'\n",
    "labels_dir = 'data/whale-detection-challenge/data/train.csv'\n",
    "\n",
    "audio_df, labels_df, test_files = load_data(train_dir, test_dir, labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87283539",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(audio_df, 'audio')\n",
    "display_random_samples(audio_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd1a46",
   "metadata": {},
   "source": [
    "### Hacemos un análisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_balance(audio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_spectrograms(audio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features_df = extract_acoustic_features(audio_df)\n",
    "display_features_boxplots(audio_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTO PCA\n",
    "flattened_mel_spectrogram = get_all_mel_spectrograms(audio_df)\n",
    "pca = PCA(n_components=2)\n",
    "spec_pca = pca.fit_transform(flattened_mel_spectrogram)\n",
    "\n",
    "plot_pca(audio_df, spec_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032c92c",
   "metadata": {},
   "source": [
    "### Arrancamos el modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d7f2a",
   "metadata": {},
   "source": [
    "### Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "14b2fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spectrogram(spec, global_min, global_max):\n",
    "    norm_spec = (spec - global_min) / (global_max - global_min + 1e-8)\n",
    "    return norm_spec\n",
    "\n",
    "def denormalize_spectrogram(norm_spec, min_val, max_val):\n",
    "    return norm_spec * (max_val - min_val + 1e-8) + min_val\n",
    "\n",
    "def compute_global_min_max(audio_df):\n",
    "    whale_specs = [get_melspectrogram(audio) for audio, label in zip(audio_df['audio'], audio_df['label']) if label == 1]\n",
    "    global_min = np.min([spec.min() for spec in whale_specs])\n",
    "    global_max = np.max([spec.max() for spec in whale_specs])\n",
    "    return global_min, global_max\n",
    "\n",
    "def get_class_weights(y):\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def get_data_loaders(train_df):\n",
    "    X = get_all_mel_spectrograms(train_df)\n",
    "    y = train_df['label'].values\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=128)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3189a94",
   "metadata": {},
   "source": [
    "### Creamos las arquitecturas necesarias para el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa88353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- MLP -------------\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[256, 128, 64], output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_layer in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=1e-5):\n",
    "        self.to(self.device)\n",
    "        y_train = train_loader.dataset.tensors[1].cpu().numpy().flatten()\n",
    "        criterion = nn.CrossEntropyLoss(weight=get_class_weights(y_train).to(self.device))\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}')\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self(inputs)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# ESTO DE ABAJO NO ANDA TODAVIA PERO ES PARA TENER\n",
    "def MLP_cross_val(train_loader:DataLoader, val_loader:DataLoader, input_dim:int, hidden_layers:list, output_dim:int, epochs:int, lr:list, weight_decay:list, regularization_term:list, early_stopping_patience:list):\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    global_min, global_max = compute_global_min_max(train_loader.dataset)\n",
    "    for lr_val in lr:\n",
    "        for wd in weight_decay:\n",
    "            for reg in regularization_term:\n",
    "                model = MLP(input_dim, hidden_layers, output_dim)\n",
    "                model.train(train_loader, val_loader, epochs=epochs, lr=lr_val, weight_decay=wd)\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                val_loss = 0.0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model = model\n",
    "                    best_params = (lr_val, wd, reg)\n",
    "    print(f'Best Validation Loss: {best_val_loss:.4f} with params: LR={best_params[0]}, WD={best_params[1]}, Reg={best_params[2]}')\n",
    "    return best_model, best_params    \n",
    "\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6d377",
   "metadata": {},
   "source": [
    "### Obtenemos los audios de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87161b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_train_df = audio_df[audio_df['clip_name'].str.contains('train')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14da827",
   "metadata": {},
   "source": [
    "### Entrenamos alternando entre una loss con pesos y cambiando la máxima frecuencia de los espectrogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d0531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MAX_FREQ = 500 HZ ---\n",
      "\n",
      "--- WEIGHTED LOSS = False ---\n",
      "Epoch 1 - AUC: 0.9143\n",
      "Epoch 2 - AUC: 0.9206\n",
      "Epoch 3 - AUC: 0.9266\n",
      "Epoch 4 - AUC: 0.9349\n",
      "Epoch 5 - AUC: 0.9384\n",
      "Epoch 6 - AUC: 0.9354\n",
      "Epoch 7 - AUC: 0.9389\n",
      "Epoch 8 - AUC: 0.9374\n",
      "Epoch 9 - AUC: 0.9432\n",
      "Epoch 10 - AUC: 0.9436\n",
      "Epoch 11 - AUC: 0.9426\n",
      "Epoch 12 - AUC: 0.9438\n",
      "Epoch 13 - AUC: 0.9408\n",
      "Epoch 14 - AUC: 0.9394\n",
      "Epoch 15 - AUC: 0.9424\n",
      "Early stopping\n",
      "AUC MLP final: 0.9438\n",
      "F1-SCORE: 0.7570\n",
      "\n",
      "--- WEIGHTED LOSS = True ---\n",
      "Epoch 1 - AUC: 0.9050\n",
      "Epoch 2 - AUC: 0.9166\n",
      "Epoch 3 - AUC: 0.9270\n",
      "Epoch 4 - AUC: 0.9266\n",
      "Epoch 5 - AUC: 0.9350\n",
      "Epoch 6 - AUC: 0.9356\n",
      "Epoch 7 - AUC: 0.9363\n",
      "Epoch 8 - AUC: 0.9385\n",
      "Epoch 9 - AUC: 0.9356\n",
      "Epoch 10 - AUC: 0.9397\n",
      "Epoch 11 - AUC: 0.9441\n",
      "Epoch 12 - AUC: 0.9456\n",
      "Epoch 13 - AUC: 0.9431\n",
      "Epoch 14 - AUC: 0.9392\n",
      "Epoch 15 - AUC: 0.9461\n",
      "Epoch 16 - AUC: 0.9466\n",
      "Epoch 17 - AUC: 0.9411\n",
      "Epoch 18 - AUC: 0.9476\n",
      "Epoch 19 - AUC: 0.9467\n",
      "Epoch 20 - AUC: 0.9459\n",
      "AUC MLP final: 0.9476\n",
      "F1-SCORE: 0.7638\n",
      "\n",
      "--- MAX_FREQ = 600 HZ ---\n",
      "\n",
      "--- WEIGHTED LOSS = False ---\n",
      "Epoch 1 - AUC: 0.9134\n",
      "Epoch 2 - AUC: 0.9208\n",
      "Epoch 3 - AUC: 0.9264\n",
      "Epoch 4 - AUC: 0.9327\n",
      "Epoch 5 - AUC: 0.9256\n",
      "Epoch 6 - AUC: 0.9362\n",
      "Epoch 7 - AUC: 0.9394\n",
      "Epoch 8 - AUC: 0.9392\n",
      "Epoch 9 - AUC: 0.9410\n",
      "Epoch 10 - AUC: 0.9329\n",
      "Epoch 11 - AUC: 0.9435\n",
      "Epoch 12 - AUC: 0.9425\n",
      "Epoch 13 - AUC: 0.9442\n",
      "Epoch 14 - AUC: 0.9454\n",
      "Epoch 15 - AUC: 0.9433\n",
      "Epoch 16 - AUC: 0.9433\n",
      "Epoch 17 - AUC: 0.9423\n",
      "Early stopping\n",
      "AUC MLP final: 0.9454\n",
      "F1-SCORE: 0.6821\n",
      "\n",
      "--- WEIGHTED LOSS = True ---\n",
      "Epoch 1 - AUC: 0.9079\n",
      "Epoch 2 - AUC: 0.9180\n",
      "Epoch 3 - AUC: 0.9239\n",
      "Epoch 4 - AUC: 0.9275\n",
      "Epoch 5 - AUC: 0.9324\n",
      "Epoch 6 - AUC: 0.9329\n",
      "Epoch 7 - AUC: 0.9375\n",
      "Epoch 8 - AUC: 0.9373\n",
      "Epoch 9 - AUC: 0.9298\n",
      "Epoch 10 - AUC: 0.9340\n",
      "Early stopping\n",
      "AUC MLP final: 0.9375\n",
      "F1-SCORE: 0.7490\n",
      "\n",
      "--- MAX_FREQ = 1000 HZ ---\n",
      "\n",
      "--- WEIGHTED LOSS = False ---\n",
      "Epoch 1 - AUC: 0.8929\n",
      "Epoch 2 - AUC: 0.9157\n",
      "Epoch 3 - AUC: 0.9250\n",
      "Epoch 4 - AUC: 0.9292\n",
      "Epoch 5 - AUC: 0.9344\n",
      "Epoch 6 - AUC: 0.9312\n",
      "Epoch 7 - AUC: 0.9424\n",
      "Epoch 8 - AUC: 0.9415\n",
      "Epoch 9 - AUC: 0.9430\n",
      "Epoch 10 - AUC: 0.9447\n",
      "Epoch 11 - AUC: 0.9421\n",
      "Epoch 12 - AUC: 0.9458\n",
      "Epoch 13 - AUC: 0.9445\n",
      "Epoch 14 - AUC: 0.9445\n",
      "Epoch 15 - AUC: 0.9457\n",
      "Early stopping\n",
      "AUC MLP final: 0.9458\n",
      "F1-SCORE: 0.7177\n",
      "\n",
      "--- WEIGHTED LOSS = True ---\n",
      "Epoch 1 - AUC: 0.9013\n",
      "Epoch 2 - AUC: 0.9113\n",
      "Epoch 3 - AUC: 0.9182\n",
      "Epoch 4 - AUC: 0.9260\n",
      "Epoch 5 - AUC: 0.9260\n",
      "Epoch 6 - AUC: 0.9369\n",
      "Epoch 7 - AUC: 0.9379\n",
      "Epoch 8 - AUC: 0.9412\n",
      "Epoch 9 - AUC: 0.9434\n",
      "Epoch 10 - AUC: 0.9402\n",
      "Epoch 11 - AUC: 0.9393\n",
      "Epoch 12 - AUC: 0.9401\n",
      "Early stopping\n",
      "AUC MLP final: 0.9434\n",
      "F1-SCORE: 0.7457\n"
     ]
    }
   ],
   "source": [
    "# nota: aca estamos overfitteando -> hay que separar en folds\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "original_max_freq = MAX_FREQ\n",
    "for mf in [500, 600, 1000]:\n",
    "    print(f'\\n--- MAX_FREQ = {mf} HZ ---')\n",
    "\n",
    "    MAX_FREQ = mf\n",
    "    train_loader, val_loader = get_data_loaders(audio_train_df)\n",
    "    X_train = train_loader.dataset.tensors[0].cpu().numpy()\n",
    "    y_train = train_loader.dataset.tensors[1].cpu().numpy().flatten()\n",
    "\n",
    "    for weighted in [False, True]:\n",
    "        print(f\"\\n--- WEIGHTED LOSS = {weighted} ---\")\n",
    "        \n",
    "        mlp = MLP(X_train.shape[1]).to(device)\n",
    "        if weighted:\n",
    "            weights = get_class_weights(y_train).to(device)\n",
    "            criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "        best_auc = 0\n",
    "        patience = 3\n",
    "        counter = 0\n",
    "\n",
    "        for epoch in range(20):\n",
    "            mlp.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(mlp(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            mlp.eval()\n",
    "            all_probs, all_targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    probs = torch.softmax(mlp(xb), dim=1)[:, 1].cpu().numpy()\n",
    "                    all_probs.extend(probs)\n",
    "                    all_targets.extend(yb.numpy())\n",
    "            auc = roc_auc_score(all_targets, all_probs)\n",
    "            print(f\"Epoch {epoch+1} - AUC: {auc:.4f}\")\n",
    "\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        print(f\"AUC MLP final: {best_auc:.4f}\")\n",
    "        \n",
    "        y_pred = [1 if p > 0.5 else 0 for p in all_probs]\n",
    "        f1 = f1_score(all_targets, y_pred)\n",
    "        print(f\"F1-SCORE: {f1:.4f}\")\n",
    "\n",
    "MAX_FREQ = original_max_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1673d",
   "metadata": {},
   "source": [
    "### Preprocesamos los datos para dárselos al modelo\n",
    "weight y maxfreq no cambio casi nada, habria q entender pq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4cb0682",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MAX_FREQ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_mel_spectrograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_train_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m audio_train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      7\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[1;32m      8\u001b[0m )\n",
      "Cell \u001b[0;32mIn[36], line 107\u001b[0m, in \u001b[0;36mget_all_mel_spectrograms\u001b[0;34m(audio_df, sampling_rate, n_fft, hop_length, n_mels)\u001b[0m\n\u001b[1;32m    105\u001b[0m mel_specs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audio_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 107\u001b[0m     mel \u001b[38;5;241m=\u001b[39m \u001b[43mget_melspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     mel_specs\u001b[38;5;241m.\u001b[39mappend(mel\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(mel_specs)\n",
      "Cell \u001b[0;32mIn[36], line 100\u001b[0m, in \u001b[0;36mget_melspectrogram\u001b[0;34m(sample, sampling_rate, n_fft, hop_length, n_mels)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_melspectrogram\u001b[39m(sample:pd\u001b[38;5;241m.\u001b[39mDataFrame, sampling_rate\u001b[38;5;241m=\u001b[39mSR, n_fft\u001b[38;5;241m=\u001b[39mN_FFT, hop_length\u001b[38;5;241m=\u001b[39mHOP_LENGTH, n_mels\u001b[38;5;241m=\u001b[39mN_MELS):\n\u001b[0;32m--> 100\u001b[0m     mel_spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_FREQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     mel_spectrogram \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpower_to_db(mel_spectrogram, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_spectrogram\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/librosa/feature/spectral.py:2148\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2135\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m _spectrogram(\n\u001b[1;32m   2136\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   2137\u001b[0m     S\u001b[38;5;241m=\u001b[39mS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2144\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[1;32m   2145\u001b[0m )\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[0;32m-> 2148\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m \u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2150\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...ft,mf->...mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, S, mel_basis, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/librosa/filters.py:239\u001b[0m, in \u001b[0;36mmel\u001b[0;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     upper \u001b[38;5;241m=\u001b[39m ramps[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m fdiff[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# .. then intersect them with each other and zero\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mminimum(lower, upper))\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(norm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslaney\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# Slaney-style mel is scaled to be approx constant energy per channel\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_FREQ = 500\n",
    "\n",
    "X = get_all_mel_spectrograms(audio_train_df)\n",
    "y = audio_train_df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba9f23",
   "metadata": {},
   "source": [
    "### Entrenamos 3 clasificadores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# MLP (PyTorch)\n",
    "# --------------------------\n",
    "mlp_torch = MLP(X_train.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlp_torch.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_torch.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    mlp_torch.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_torch(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    mlp_torch.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            outputs = mlp_torch(xb)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += yb.size(0)\n",
    "            correct += (predicted == yb).sum().item()\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy (MLP): {val_acc:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Random Forest\n",
    "# --------------------------\n",
    "def train_random_forest(X_train, y_train, X_val, y_val):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    print(f\"Validation Accuracy (Random Forest): {acc:.4f}\")\n",
    "    return rf\n",
    "\n",
    "# --------------------------\n",
    "# Gradient Boosting\n",
    "# --------------------------\n",
    "def train_gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    preds = gb.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    print(f\"Validation Accuracy (Gradient Boosting): {acc:.4f}\")\n",
    "    return gb\n",
    "\n",
    "# --------------------------\n",
    "# Ejecutar modelos sklearn\n",
    "# --------------------------\n",
    "# Convertimos a numpy si venís usando tensores\n",
    "X_train_np = X_train_tensor.numpy()\n",
    "y_train_np = y_train_tensor.numpy()\n",
    "X_val_np = X_val_tensor.numpy()\n",
    "y_val_np = y_val_tensor.numpy()\n",
    "\n",
    "rf_model = train_random_forest(X_train_np, y_train_np, X_val_np, y_val_np)\n",
    "gb_model = train_gradient_boosting(X_train_np, y_train_np, X_val_np, y_val_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e78cf",
   "metadata": {},
   "source": [
    "### Visualizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Asegurate de que y_val_np esté definido\n",
    "# (ya lo tenés si ejecutaste los bloques anteriores)\n",
    "# y que las etiquetas estén en formato binario (0/1)\n",
    "\n",
    "# 1. MLP - predicción de probabilidades\n",
    "mlp_torch.eval()\n",
    "mlp_probs = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        outputs = mlp_torch(xb)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # clase positiva\n",
    "        mlp_probs.extend(probs.cpu().numpy())\n",
    "mlp_probs = np.array(mlp_probs)\n",
    "mlp_auc = roc_auc_score(y_val_np, mlp_probs)\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_val_np, mlp_probs)\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_probs = rf_model.predict_proba(X_val_np)[:, 1]\n",
    "rf_auc = roc_auc_score(y_val_np, rf_probs)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_val_np, rf_probs)\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "gb_probs = gb_model.predict_proba(X_val_np)[:, 1]\n",
    "gb_auc = roc_auc_score(y_val_np, gb_probs)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_val_np, gb_probs)\n",
    "\n",
    "# 4. Plot all ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_mlp, tpr_mlp, label=f'MLP (AUC = {mlp_auc:.3f})')\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_auc:.3f})')\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {gb_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=0.8)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b712da5",
   "metadata": {},
   "source": [
    "### Entrenamos modelos generativos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c64d1",
   "metadata": {},
   "source": [
    "#### Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b54419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTO [ NO BORRAR ]\n",
    "\n",
    "# Preparo los espectrogramas para enchufarselos al vae\n",
    "#   - grafico 5 muestras pre y post normalizar\n",
    "print(audio_df)\n",
    "\n",
    "original_spec_samples = []\n",
    "normalized_samples = []\n",
    "denormalized_samples = []\n",
    " \n",
    "global_min, global_max = compute_global_min_max(audio_df)\n",
    "\n",
    "whale_audio_samples = audio_df[audio_df['label'] == 1].sample(5)['audio']\n",
    "\n",
    "for sample in whale_audio_samples:\n",
    "    mel_spec = get_melspectrogram(sample)\n",
    "    original_spec_samples.append(mel_spec)\n",
    "\n",
    "    normalized_mel_spec = normalize_spectrogram(mel_spec, global_min, global_max)\n",
    "    normalized_samples.append(normalized_mel_spec)\n",
    "\n",
    "    denormalized_mel_spec = denormalize_spectrogram(normalized_mel_spec, global_min, global_max)\n",
    "    denormalized_samples.append(denormalized_mel_spec)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 8))\n",
    "\n",
    "for i in range(5):\n",
    "    # Original spectrogram\n",
    "    img0 = librosa.display.specshow(original_spec_samples[i], sr=SR, hop_length=HOP_LENGTH, ax=axes[0, i], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[0, i].set_title(f'Original {i+1}')\n",
    "    axes[0, i].set_xticks([])\n",
    "    axes[0, i].set_yticks([])\n",
    "\n",
    "    # Normalized spectrogram\n",
    "    img1 = librosa.display.specshow(normalized_samples[i], sr=SR, hop_length=HOP_LENGTH, ax=axes[1, i], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[1, i].set_title(f'Normalized {i+1}')\n",
    "    axes[1, i].set_xticks([])\n",
    "    axes[1, i].set_yticks([])\n",
    "\n",
    "    # Denormalized spectrogram\n",
    "    img2 = librosa.display.specshow(denormalized_samples[i], sr=SR, hop_length=HOP_LENGTH, ax=axes[2, i], x_axis='time', y_axis='hz', cmap='magma', fmax=MAX_FREQ)\n",
    "    axes[2, i].set_title(f'Denormalized {i+1}')\n",
    "    axes[2, i].set_xticks([])\n",
    "    axes[2, i].set_yticks([])\n",
    "\n",
    "axes[0, 0].set_ylabel('Original')\n",
    "axes[1, 0].set_ylabel('Normalized')\n",
    "axes[2, 0].set_ylabel('Denormalized')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(original_spec_samples[-1])\n",
    "print(normalized_samples[-1])\n",
    "print(denormalized_samples[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO TODOS LOS AUDIOS A UN ESPECTROGRAMA Y NORMALIZO\n",
    "\n",
    "mel_specs = []\n",
    "for whale_audio in audio_df[audio_df['label'] == 1]['audio']:\n",
    "    mel = get_melspectrogram(whale_audio)\n",
    "    mel_norm = normalize_spectrogram(mel, global_min, global_max)\n",
    "    mel_specs.append(mel_norm.flatten())\n",
    "X = np.array(mel_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abe63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARO LOS DATOS PARA ENCHUFARSELOS AL VAE\n",
    "\n",
    "X = np.array(mel_specs)  # shape: (num_samples, N_MELS * time_steps)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(X_tensor), batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIATIONAL AUTOENCODER MODEL\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "    def loss(self, x, x_hat, mu, logvar):\n",
    "        recon_loss = nn.functional.mse_loss(x_hat, x, reduction='mean')\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kl_div\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(X.shape[1], latent_dim=32).to(device)  # Use all data, not X_train\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "    for xb_tuple in train_loader:  # xb_tuple is a tuple (xb,)\n",
    "        xb = xb_tuple[0].to(device)\n",
    "        x_hat, mu, logvar = vae(xb)\n",
    "        loss = vae.loss(xb, x_hat, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)  # accumulate per sample\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERO MUESTRAS SINTÉTICAS\n",
    "\n",
    "MAX_FREQ = 600 # ARREGLAR ESTO\n",
    "\n",
    "vae.eval()\n",
    "num_samples = 3\n",
    "latent_dim = 32  # Must match your VAE's latent_dim\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    generated = vae.decode(z).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, num_samples, figsize=(12, 4))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Reshape to (N_MELS, time_steps)\n",
    "    spec_norm = generated[i].reshape((N_MELS, -1))\n",
    "    # Denormalize\n",
    "    spec = denormalize_spectrogram(spec_norm, global_min, global_max)\n",
    "    librosa.display.specshow(spec, sr=SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='hz', cmap='magma', ax=axes[i], fmax=MAX_FREQ)\n",
    "    axes[i].set_title(f\"Synthetic Sample {i+1}\")\n",
    "    axes[i].set_ylim([0, MAX_FREQ])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESCUCHAMOS LAS MUESTRAS SINTÉTICAS [ NO PROBE ESTO TODAVIA ]\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "for i in range(num_samples):\n",
    "    spec_norm = generated[i].reshape((N_MELS, -1))\n",
    "    spec = denormalize_spectrogram(spec_norm, global_min, global_max)\n",
    "    # Invert dB to power\n",
    "    spec_power = librosa.db_to_power(spec)\n",
    "    # Invert Mel-spectrogram to waveform\n",
    "    audio = librosa.feature.inverse.mel_to_audio(\n",
    "        spec_power, \n",
    "        sr=SR, \n",
    "        n_fft=N_FFT, \n",
    "        hop_length=HOP_LENGTH, \n",
    "        n_iter=32, \n",
    "        fmax=MAX_FREQ\n",
    "    )\n",
    "    print(f\"Synthetic Sample {i+1}:\")\n",
    "    display(Audio(audio, rate=SR*1.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BÚSQUEDA DE HIPERPARÁMETROS [ CAMBIAR PARA QUE SE PUEDA PROBAR DISTINTAS ARQUITECTURAS ]\n",
    "\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "\n",
    "def train_vae(X, latent_dim, lr, epochs, batch_size, device):\n",
    "    vae = VAE(X.shape[1], latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    vae.train()\n",
    "    for epoch in range(epochs):\n",
    "        for xb_tuple in loader:\n",
    "            xb = xb_tuple[0].to(device)\n",
    "            x_hat, mu, logvar = vae(xb)\n",
    "            loss = vae.loss(xb, x_hat, mu, logvar)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return vae\n",
    "\n",
    "def evaluate_vae(vae, X, device):\n",
    "    vae.eval()\n",
    "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=128)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb_tuple in loader:\n",
    "            xb = xb_tuple[0].to(device)\n",
    "            x_hat, mu, logvar = vae(xb)\n",
    "            loss = vae.loss(xb, x_hat, mu, logvar)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "# Example grid\n",
    "param_grid = {\n",
    "    'latent_dim': [8, 16, 32],\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'epochs': [20, 40],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "results = []\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    val_losses = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        vae = train_vae(X_train, **params, device=device)\n",
    "        val_loss = evaluate_vae(vae, X_val, device=device)\n",
    "        val_losses.append(val_loss)\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    print(f\"Params: {params}, Avg Val Loss: {avg_loss:.4f}\")\n",
    "    results.append((params, avg_loss))\n",
    "\n",
    "# Find best params\n",
    "best_params = min(results, key=lambda x: x[1])\n",
    "print(\"Best params:\", best_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74451cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Visualización con PCA\n",
    "# --------------------------\n",
    "vae.eval()\n",
    "all_mu = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        mu, _ = vae.encode(xb)\n",
    "        all_mu.append(mu.cpu().numpy())\n",
    "        all_labels.append(yb.numpy())\n",
    "\n",
    "all_mu = np.concatenate(all_mu, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "z_pca = pca.fit_transform(all_mu)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(z_pca[:, 0], z_pca[:, 1], c=all_labels, cmap='coolwarm', alpha=0.7)\n",
    "plt.title(\"VAE Latent Space (PCA)\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(scatter, label=\"Clase\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15179693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=SEED)\n",
    "z_tsne = tsne.fit_transform(all_mu)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=all_labels, cmap='coolwarm', alpha=0.7)\n",
    "plt.title(\"VAE Latent Space (t-SNE)\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.colorbar(scatter, label=\"Clase\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa945e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === FUNCIONES PROPIAS REQUERIDAS ===\n",
    "# - get_melspectrogram(audio)\n",
    "# - normalize_spectrogram(mel, global_min, global_max)\n",
    "\n",
    "# === PREPROCESADO ===\n",
    "mel_specs_ballenas = []\n",
    "mel_specs_no_ballenas = []\n",
    "\n",
    "for _, row in audio_df.iterrows():\n",
    "    mel = get_melspectrogram(row['audio'])  # audio: waveform\n",
    "    mel_norm = normalize_spectrogram(mel, global_min, global_max)\n",
    "    mel_flat = mel_norm.flatten()\n",
    "\n",
    "    if row['label'] == 1:\n",
    "        mel_specs_ballenas.append(mel_flat)\n",
    "    else:\n",
    "        mel_specs_no_ballenas.append(mel_flat)\n",
    "\n",
    "# Convertimos a arrays\n",
    "X_ballenas     = np.array(mel_specs_ballenas)  # (N1, N_MELS * T)\n",
    "X_no_ballenas  = np.array(mel_specs_no_ballenas)  # (N2, N_MELS * T)\n",
    "\n",
    "# Unimos todo\n",
    "X_total = np.vstack([X_ballenas, X_no_ballenas])  # (N1+N2, ...)\n",
    "y_total = np.array([1]*len(X_ballenas) + [0]*len(X_no_ballenas))\n",
    "\n",
    "# Para PyTorch\n",
    "X_tensor_total = torch.tensor(X_total, dtype=torch.float32)\n",
    "y_tensor_total = torch.tensor(y_total, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Para VAE individual (solo ballenas)\n",
    "X_tensor_ballenas = torch.tensor(X_ballenas, dtype=torch.float32)\n",
    "train_loader_ballenas = DataLoader(TensorDataset(X_tensor_ballenas), batch_size=128, shuffle=True)\n",
    "\n",
    "# Para CVAE (todo con labels)\n",
    "train_loader_total = DataLoader(TensorDataset(X_tensor_total, y_tensor_total), batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, cond_dim=1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        x_in = torch.cat([x, y], dim=1)\n",
    "        h = self.encoder(x_in)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        z_in = torch.cat([z, y], dim=1)\n",
    "        return self.decoder(z_in)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z, y)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "    def loss(self, x, x_hat, mu, logvar):\n",
    "        recon = nn.functional.mse_loss(x_hat, x, reduction='mean')\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon + kl\n",
    "\n",
    "# Preparar datos\n",
    "X_tensor = torch.tensor(X_total, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_total.reshape(-1, 1), dtype=torch.float32)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "cvae = CVAE(X_total.shape[1], latent_dim=32).to(device)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "\n",
    "# Entrenar\n",
    "for epoch in range(50):\n",
    "    cvae.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        x_hat, mu, logvar = cvae(xb, yb)\n",
    "        loss = cvae.loss(xb, x_hat, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    print(f\"[CVAE] Epoch {epoch+1}, Loss: {total_loss / len(loader.dataset):.4f}\")\n",
    "\n",
    "\n",
    "vae_ballena = VAE(X_ballenas.shape[1], latent_dim=32).to(device)\n",
    "vae_noballena = VAE(X_no_ballenas.shape[1], latent_dim=32).to(device)\n",
    "\n",
    "def train_vae(vae, X_data):\n",
    "    loader = DataLoader(TensorDataset(torch.tensor(X_data, dtype=torch.float32)), batch_size=128, shuffle=True)\n",
    "    optim_vae = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "    for epoch in range(50):\n",
    "        vae.train()\n",
    "        total_loss = 0\n",
    "        for (xb,) in loader:\n",
    "            xb = xb.to(device)\n",
    "            x_hat, mu, logvar = vae(xb)\n",
    "            loss = vae.loss(xb, x_hat, mu, logvar)\n",
    "            optim_vae.zero_grad()\n",
    "            loss.backward()\n",
    "            optim_vae.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        print(f\"[VAE {'ballena' if vae == vae_ballena else 'no-ballena'}] Epoch {epoch+1}, Loss: {total_loss / len(loader.dataset):.4f}\")\n",
    "\n",
    "train_vae(vae_ballena, X_ballenas)\n",
    "train_vae(vae_noballena, X_no_ballenas)\n",
    "\n",
    "def get_latent_mu(vae, X, device):\n",
    "    vae.eval()\n",
    "    mu_list = []\n",
    "    loader = DataLoader(torch.tensor(X, dtype=torch.float32), batch_size=128)\n",
    "    with torch.no_grad():\n",
    "        for xb in loader:\n",
    "            xb = xb.to(device)\n",
    "            mu, _ = vae.encode(xb)\n",
    "            mu_list.append(mu.cpu().numpy())\n",
    "    return np.vstack(mu_list)\n",
    "\n",
    "# CVAE: usar label como condición\n",
    "X_tensor = torch.tensor(X_total, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_total.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "cvae.eval()\n",
    "with torch.no_grad():\n",
    "    mu_cvae, _ = cvae.encode(X_tensor, y_tensor)\n",
    "    mu_cvae = mu_cvae.cpu().numpy()\n",
    "\n",
    "# VAE individual\n",
    "mu_ballena = get_latent_mu(vae_ballena, X_ballenas, device)\n",
    "mu_noballena = get_latent_mu(vae_noballena, X_no_ballenas, device)\n",
    "\n",
    "# Unimos y visualizamos\n",
    "mu_2vae = np.vstack([mu_ballena, mu_noballena])\n",
    "y_2vae = np.array([1]*len(mu_ballena) + [0]*len(mu_noballena))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_latent(z, labels, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap='coolwarm', alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Clase\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# PCA y t-SNE para ambos métodos\n",
    "plot_latent(PCA(2).fit_transform(mu_cvae), y_total, \"CVAE Latent Space (PCA)\")\n",
    "plot_latent(TSNE(2, random_state=42).fit_transform(mu_cvae), y_total, \"CVAE Latent Space (t-SNE)\")\n",
    "\n",
    "plot_latent(PCA(2).fit_transform(mu_2vae), y_2vae, \"Dos VAEs Latent Space (PCA)\")\n",
    "plot_latent(TSNE(2, random_state=42).fit_transform(mu_2vae), y_2vae, \"Dos VAEs Latent Space (t-SNE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# GAN parameters\n",
    "latent_dim = 32\n",
    "input_dim = X.shape[1]  # Flattened mel spectrogram size\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Sigmoid()  # Output normalized [0,1]\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate models\n",
    "generator = Generator(latent_dim, input_dim).to(device)\n",
    "discriminator = Discriminator(input_dim).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "lr = 2e-4\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Example training loop (1 epoch)\n",
    "for epoch in range(epochs):  # Replace with more epochs for real training\n",
    "    for xb_tuple in train_loader:\n",
    "        real_data = xb_tuple[0].to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        fake_data = generator(z)\n",
    "        real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "        d_real = discriminator(real_data)\n",
    "        d_fake = discriminator(fake_data.detach())\n",
    "        d_loss_real = criterion(d_real, real_labels)\n",
    "        d_loss_fake = criterion(d_fake, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        fake_data = generator(z)\n",
    "        d_fake = discriminator(fake_data)\n",
    "        g_loss = criterion(d_fake, real_labels)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
